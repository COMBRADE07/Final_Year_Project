{% extends 'navbar.html' %}
{% load static %}
{% block title%}
    Home Page
{% endblock title%}

{% block cssfiles %}
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
<link rel="stylesheet" href={% static 'css/home.css' %}>
{% endblock cssfiles %}

{% block content%}
       <div class="container">
        <h2>Random Forest Algorithm:</h2>

        <h2>Description</h2>
        <p>Random Forest is a popular machine learning algorithm that belongs to the ensemble learning family. It combines multiple decision trees to make predictions. Each decision tree in the Random Forest is built independently using a subset of the training data and a random subset of features. The final prediction is made by aggregating the predictions of all the individual trees.</p>
        <div class="image-container">
            <img src="{% static 'images/algorithm.png' %}" >
        </div>
        <h2>Algorithm Steps</h2>
        <ol>
            <li><strong>Random selection of data:</strong> Random Forest randomly selects a subset of the training data, called a bootstrap sample, to build each decision tree. This process is known as bagging (bootstrap aggregating).</li>
            <li><strong>Random selection of features:</strong> For each node in the decision tree, a random subset of features is considered to determine the best split. This helps to reduce the correlation between the trees and leads to more diverse and robust models.</li>
            <li><strong>Building decision trees:</strong> Each decision tree is grown using the selected subset of data and features. The trees are built by recursively partitioning the data based on feature thresholds, aiming to minimize impurity or maximize information gain.</li>
            <li><strong>Aggregating predictions:</strong> When making predictions, each decision tree in the Random Forest independently predicts the outcome, and the final prediction is determined by aggregating the individual tree predictions. The most common aggregation methods are majority voting for classification problems and averaging for regression problems.</li>
        </ol>

        <div >
            <h2>Advantages</h2>
            <ol>
                <li class="list-item"><strong>Robust to overfitting:</strong> Random Forest reduces the risk of overfitting by averaging the predictions of multiple decision trees. Each tree is trained on a subset of data, leading to a diverse set of models that collectively make more accurate predictions.</li>
                <li class="list-item"><strong>Handles high-dimensional data:</strong> Random Forest performs well even when the number of features is large compared to the number of samples. It can handle high-dimensional data and identify the most relevant features for prediction.</li>
                <li class="list-item"><strong>Handles missing values and outliers:</strong> Random Forest can handle missing values and outliers in the data. It automatically takes care of missing values during the tree building process and is not sensitive to outliers.</li>
                <li class="list-item"><strong>Provides feature importance:</strong> Random Forest can measure the importance of each feature in the prediction task. This information can be useful for feature selection and understanding the underlying relationships in the data.</li>
            </ol>
        </div>

        <div class="container">
            <h2>Disadvantages</h2>
            <ol>
                <li class="list-item"><strong>Lack of interpretability:</strong> Random Forest models are not easily interpretable compared to simpler models like decision trees. The combined effect of multiple trees makes it difficult to understand the exact decision-making process.</li>
                <li class="list-item"><strong>Computationally expensive:</strong> Training a Random Forest with a large number of trees and features can be computationally expensive, especially for large datasets. However, the training process can be parallelized to some extent to mitigate this issue.</li>
                <li class="list-item"><strong>Biased towards features with more levels:</strong> Random Forest tends to be biased towards features with more levels or categories. Features with fewer levels may have less influence on the predictions.</li>
                <li class="list-item"><strong>May overfit noisy data:</strong> Random Forest can overfit noisy or irrelevant features in the data, especially if the number of trees is high. Proper feature selection and regularization techniques can help mitigate this issue.</li>
            </ol>
        </div>
    </div>
        <br><br><br><br>
        <div class="container">
        <h2>Random Forest Regressor Algorithm:</h2>

        <h2>Description</h2>
        <p>Random Forest Regressor is an ensemble learning algorithm that belongs to the family of decision tree-based methods. It builds multiple decision trees during training and merges them together to get a more accurate and stable prediction. Here's an explanation of how the algorithm works along with its advantages and disadvantages:</p>
        <div class="image-container">
            <img src="{% static 'images/random_forest.png' %}" >
        </div>
        <h2>Algorithm usage</h2>
        <ol>
            <li><strong>Bootstrap Sampling (Bagging):</strong> Random Forest starts by creating multiple random subsets of the training data through a process called bootstrap sampling. This involves randomly selecting data points with replacement, allowing some data points to appear in the sample multiple times, while others may not appear at all.

            </li> <li><strong>Decision Tree Construction:</strong> A decision tree is constructed for each subset of the data. However, during the construction of each tree, only a random subset of features is considered at each split. This introduces an element of randomness and diversity among the trees.

</li> <li><strong>Voting (Regression):</strong> Once all the trees are constructed, predictions are made by each tree. For regression tasks, the predictions of all trees are averaged to get the final prediction. This averaging helps to reduce overfitting and improves the overall performance of the model.
        </ol>

        <div >
            <h2>Advantages</h2>
            <ol>
                <li><strong>High Accuracy:</strong> Random Forest tends to provide high accuracy in both classification and regression tasks. The averaging of multiple trees helps to mitigate overfitting, resulting in a more robust model.

</li> <li><strong>Handles Non-linearity and Complex Relationships:</strong> Random Forest can capture complex relationships and non-linear patterns in the data, making it suitable for a wide range of applications.

</li> <li><strong>Feature Importance:</strong> The algorithm provides a feature importance measure, which helps in identifying the most relevant features contributing to the prediction. This can be useful for feature selection and understanding the underlying data patterns.

</li> <li><strong>Robust to Outliers:</strong> Random Forest is robust to outliers and noise in the data due to the ensemble averaging. Outliers in individual trees are likely to have less impact on the overall prediction.

</li> <li><strong>Little Hyperparameter Tuning Required:</strong> Random Forests are less sensitive to hyperparameter choices compared to individual decision trees. They often perform well with default hyperparameter settings.
            </li></ol>
        </div>

        <div class="container">
            <h2>Disadvantages</h2>
            <ol>
                <li><strong>Computational Complexity:</strong> Training a large number of decision trees can be computationally expensive, especially for large datasets and a high number of trees. This can be a limitation in real-time applications.

</li> <li><strong>Less Interpretability:</strong> While Random Forest provides feature importance, the individual trees in the forest are often complex and hard to interpret. Understanding the decision-making process might be challenging.

</li> <li><strong>Memory Consumption:</strong> Storing multiple trees can consume a significant amount of memory, making Random Forest less suitable for deployment in memory-constrained environments.

</li> <li><strong>Overfitting on Noisy Data:</strong> While Random Forest is robust to outliers, it can still overfit on noisy data, especially if the noise is present in a large number of trees in the forest.

</li> <li><strong>Bias in Imbalanced Datasets:</strong> Random Forest can be biased towards the majority class in imbalanced datasets, although this can be mitigated to some extent by adjusting class weights.

            </ol>
        </div>
    </div>
{% endblock content%}

